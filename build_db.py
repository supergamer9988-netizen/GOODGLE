import json
import gzip # <--- à¹€à¸žà¸´à¹ˆà¸¡à¸•à¸±à¸§à¸™à¸µà¹‰
import requests
from bs4 import BeautifulSoup

# à¸£à¸²à¸¢à¸Šà¸·à¹ˆà¸­à¹€à¸§à¹‡à¸šà¸—à¸µà¹ˆà¸ˆà¸° Index (à¹ƒà¸ªà¹ˆà¹€à¸žà¸´à¹ˆà¸¡à¹„à¸”à¹‰à¹„à¸¡à¹ˆà¸ˆà¸³à¸à¸±à¸”)
database = []

# ðŸ”¥ à¹‚à¸«à¸¥à¸”à¸£à¸²à¸¢à¸à¸²à¸£à¹€à¸§à¹‡à¸šà¸ˆà¸²à¸à¹„à¸Ÿà¸¥à¹Œ sites.txt (à¹ƒà¸ªà¹ˆà¸à¸µà¹ˆà¹€à¸§à¹‡à¸šà¸à¹‡à¹„à¸”à¹‰ à¹„à¸¡à¹ˆà¸ˆà¸³à¸à¸±à¸”)
try:
    with open('sites.txt', 'r', encoding='utf-8') as f:
        urls = [line.strip() for line in f if line.strip()]
    print(f"ðŸ“‚ à¹‚à¸«à¸¥à¸”à¹€à¸›à¹‰à¸²à¸«à¸¡à¸²à¸¢à¸ªà¸³à¹€à¸£à¹‡à¸ˆ: {len(urls)} à¹€à¸§à¹‡à¸šà¹„à¸‹à¸•à¹Œ")
except FileNotFoundError:
    print("âŒ à¹„à¸¡à¹ˆà¸žà¸šà¹„à¸Ÿà¸¥à¹Œ sites.txt! à¸à¸£à¸¸à¸“à¸²à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œà¸™à¸µà¹‰à¹à¸¥à¸°à¹ƒà¸ªà¹ˆ URL à¸šà¸£à¸£à¸—à¸±à¸”à¸¥à¸° 1 à¹€à¸§à¹‡à¸š")
    urls = []

print("ðŸ•·ï¸ à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥...")
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

for url in urls:
    try:
        print(f"à¸à¸³à¸¥à¸±à¸‡à¸­à¹ˆà¸²à¸™: {url}")
        # à¹€à¸žà¸´à¹ˆà¸¡ headers à¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸·à¸­à¸™à¸„à¸™à¸ˆà¸£à¸´à¸‡à¹† à¸¡à¸²à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸”
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Language': 'en-US,en;q=0.9',
        }
        response = requests.get(url, headers=headers, timeout=10) # à¹€à¸žà¸´à¹ˆà¸¡ timeout à¹€à¸›à¹‡à¸™ 10 à¸§à¸´
        
        if response.status_code != 200:
            print(f"âš ï¸ Skip {url}: Status {response.status_code}")
            continue

        response.encoding = 'utf-8' # à¸šà¸±à¸‡à¸„à¸±à¸š utf-8 à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¸ à¸²à¸©à¸²à¸•à¹ˆà¸²à¸‡à¸”à¸²à¸§
        soup = BeautifulSoup(response.content, 'html.parser')
        
        title = soup.title.string if soup.title else url
        # à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸¡à¸²à¹à¸„à¹ˆ 500 à¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£à¹€à¸žà¸·à¹ˆà¸­à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡
        text = " ".join([p.text for p in soup.find_all('p')])[:500]
        
        database.append({
            "url": url,
            "title": title,
            "snippet": text
        })
    except Exception as e:
        print(f"âŒ Error {url}: {e}")

print(f"ðŸ“¦ à¸à¸³à¸¥à¸±à¸‡à¸šà¸µà¸šà¸­à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ {len(database)} à¸£à¸²à¸¢à¸à¸²à¸£...")

# à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸›à¹‡à¸™à¹„à¸Ÿà¸¥à¹Œ JSON à¹à¸šà¸šà¸šà¸µà¸šà¸­à¸±à¸” (GZIP)
with gzip.open("database.json.gz", "wt", encoding="utf-8") as f:
    # separators=(',', ':') à¸„à¸·à¸­à¸à¸²à¸£à¸¥à¸šà¹€à¸§à¹‰à¸™à¸§à¸£à¸£à¸„à¸—à¸´à¹‰à¸‡à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¹„à¸Ÿà¸¥à¹Œà¹€à¸¥à¹‡à¸à¸ªà¸¸à¸”
    json.dump(database, f, separators=(',', ':'))

print(f"âœ… à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œ 'database.json.gz' à¸ªà¸³à¹€à¸£à¹‡à¸ˆ! (à¸šà¸µà¸šà¸­à¸±à¸”à¹€à¸£à¸µà¸¢à¸šà¸£à¹‰à¸­à¸¢)")
print("ðŸ‘‰ à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸•à¹ˆà¸­à¹„à¸›: à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œà¸™à¸µà¹‰à¸‚à¸¶à¹‰à¸™ archive.org")
